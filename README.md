# Passive scrape

Work in progress experimental web scraping and analysis project.

## How it works

Currently, a web extension analyses open tabs and saves their DOM as HTML if
the viewed URL is among a set of tracked URLs. Extracted HTML is saved in a
database for further processing along with the list of tracked URLs.

The current implementation is a proof of concept, it lacks many planned features
and cannot be used for the intended purposes. The code is not well tested, it
has only been written with the concern of not breaking on the developer's
environment which, for reference, is postgresql and firefox developer edition.

## Theory

The goal of this project is to create a system that is able to passively and, in
theory, undetected scrape and analyse web pages that a user has browsed.

Scraping ought to occur directly on a browser instance used for day to day
activity to get a "realistic" view of page that takes into account things such
as authentication. The undetectability originates from this method as it
should be impossible to tell apart a user that is passively scraping and one
that isn't.

The intent is to collect all state that a web page uses during its operation,
everything from the response HTML, linked resources such as style sheets and
scripts, network request headers and bodies, both in response and request,
cookies and any other data a page may require.

With said state, a web page can automatically, and by extent manually, be poked
and prodded for data it contained in a given time frame without concern for the
outside world. Analysis can be done in complete isolation and without
interference.

In addition to information that can be gained by traditional scraping
techniques, this methodology promises new sorts of insights such as API
inference, machine learning directed semantic extraction and vulnerability
detection. Chief to these abilities is the analysis of how external data flows
through scripts. For a concrete example, say a given JSON response contains some
field that holds a number whose meaning is unknown. By observing the checks and
mutations a script does on that number, its meaning and function can be
inferred, especially if it plays a role in what is displayed on the DOM.

The biggest downside to this methodology is the use of a real human user. A
possible remedy is a machine learning system that mimics genuine user
interaction from a small seed dataset generated by a human. The primitive ML
agent can then be used to collect more data during which further refinement can
be conducted by say, punishing the agent if it encounters or fails a CAPTCHA or
by rewarding novel or useful data collection.

The value of the extracted information via the proposals above is questionable,
and its tractability more so. Ultimately, everything is this section should be
taken as science fiction as, to the developer's limited knowledge, none of it
has been manifested.

